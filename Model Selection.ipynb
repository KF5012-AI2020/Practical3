{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "In the previous notebook, we saw how increasing the degree of the polynomial features (i.e. increasing the complexity of the model) lead to a better fit of the (non-linear) data. In this notebook, we will explore what happens when we increase the degree of the polynomial features (i.e. the complexity of the model) even further, and will examine the impact this has on the predictive performance of the regression model. \n",
    "\n",
    "Tuning the degree of the polynomial, is an example of *model selection* which is how we describe the method of tuning the parameters of a model to improve performance.\n",
    "\n",
    "Let's imprt the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting training data\n",
    "For this example, we are going to use a smaller dataset with a lower number of samples.\n",
    "\n",
    "Let's load the data and create a simple scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonlinear = pd.read_csv('Data/nonlinear_small.csv', sep=',', header=0)\n",
    "df_nonlinear.plot.scatter(x='X', y='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's prepare the data, split into a 50:50 train and test split, and visualise the training data with black circles and the test data with blue crosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "y = df_nonlinear['y']\n",
    "X = df_nonlinear['X'][:, np.newaxis]\n",
    "X=(X-X.mean())/X.std()\n",
    "\n",
    "# Split into 50:50 training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.5, random_state=0)\n",
    "\n",
    "# Visualise both\n",
    "ax = pd.DataFrame({'x':X_train[:,0],'y':y_train}).plot.scatter(x='x', y='y',c='k');\n",
    "pd.DataFrame({'x':X_test[:,0],'y':y_test}).plot.scatter(x='x', y='y',ax=ax,c='b',marker='x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us create polynomial features using a polynomial feature map of degree $5$. Then let's train a linear regression model on the training data, and visualise the regression model using predictions on a linear space of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features of degree 5\n",
    "degree = 5\n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# Fit linear regression model\n",
    "lmod = LinearRegression()\n",
    "lmod.fit(X_poly, y_train)\n",
    "\n",
    "# Create lin space for plotting regression model\n",
    "X_linspace = np.linspace(np.min(X), np.max(X), num=50)[:, np.newaxis]\n",
    "y_linspace = lmod.predict(poly.transform(X_linspace))\n",
    "\n",
    "# Visualise everything\n",
    "ax = pd.DataFrame({'x':X_train[:,0],'y':y_train}).plot.scatter(x='x', y='y',c='k')\n",
    "pd.DataFrame({'x':X_test[:,0],'y':y_test}).plot.scatter(x='x', y='y',ax=ax ,c='b',marker='x')\n",
    "pd.DataFrame({'x':X_linspace[:,0],'y':y_linspace}).plot(x='x', y='y',ax=ax ,c='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well do you think the above regression model fits the training data (black dots)? How well do you think the above regression fits the test data (blue crosses)?\n",
    "\n",
    "What is happening in the above figure is a case of *over fitting*. Indeed, we could increase the degree of the model further until we fit all the training data points exactly, but this would likely lead to a large error on the test set.\n",
    "\n",
    "We can avoid over-fitting the training data, but the problem becomes choosing the degree of the polynomial feature so that the model performs best on new data. \n",
    "\n",
    "Let us try this by training the model on the training data for different degrees of polynomial, testing the resulting model on the test data, and storing the RMSE of each model so we can analyse the performance of each model after the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection for polynomial degree\n",
    "Let's now explore model selection in the context of choosing the degree of the polynomial feature map.\n",
    "\n",
    "First, let's specify the degrees to be tested and create an array for storing the RMSE for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [0, 1, 2, 3, 4, 5, 6]\n",
    "rmses = np.zeros((len(degrees),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's iterate over the different degrees, and for each degree:\n",
    "* train the model on the training set\n",
    "* test the model on the test set\n",
    "* store the RMSE of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for degree in degrees:\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    poly.fit(X_train)\n",
    "    \n",
    "    lmod = LinearRegression()\n",
    "    lmod.fit(poly.transform(X_train), y_train)\n",
    "    \n",
    "    y_pred = lmod.predict(poly.transform(X_test))\n",
    "    rmses[degree] = (np.sqrt(mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the degree versus the RMSE to see which degree of polynomial performs best on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'degree':degrees,'rmse':rmses}).set_index('degree')['rmse'].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the model with polynomial degree of $4$ has the lowest RMSE on this particular test set. \n",
    "\n",
    "Let's plot predictions of for the *best* regression model, to see if the result looks any better than the previous model with polynomial degree of $5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 4\n",
    "poly = PolynomialFeatures(degree)\n",
    "X_poly = poly.fit_transform(X_train)\n",
    "\n",
    "# Fit linear regression model\n",
    "lmod = LinearRegression()\n",
    "lmod.fit(X_poly, y_train)\n",
    "\n",
    "# Create lin space for plotting regression model\n",
    "X_linspace = np.linspace(np.min(X), np.max(X), num=50)[:, np.newaxis]\n",
    "y_linspace = lmod.predict(poly.transform(X_linspace))\n",
    "\n",
    "\n",
    "ax = pd.DataFrame({'x':X_train[:,0],'y':y_train}).plot.scatter(x='x', y='y',c='k')\n",
    "pd.DataFrame({'x':X_test[:,0],'y':y_test}).plot.scatter(x='x', y='y',ax=ax ,c='b',marker='x')\n",
    "pd.DataFrame({'x':X_linspace[:,0],'y':y_linspace}).plot(x='x', y='y',ax=ax ,c='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think? Better than the model with polynomial degree $5$?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
