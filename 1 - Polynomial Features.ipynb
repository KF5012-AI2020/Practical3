{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "Sometimes the best attributes to input into a machine learning algorithm are not the variables measured in the data, but new variables derived from the variables measured in the data. Such derived variables are called *features*, and in this notebook we will introduce different types of features and explore the art and science of good *feature engineering*.\n",
    "\n",
    "Let's begin by importing the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a *linear* model to *non-linear* data\n",
    "Let's begin with a some 2 dimensional data, where we think there might be a non-linear relationship between atrribute $X$ and target $y$.\n",
    "\n",
    "Run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nonlinear = pd.read_csv('Data/nonlinear.csv', sep=',', header=0)\n",
    "df_nonlinear.plot.scatter(x='X', y='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to fit a linear regression model. Using tools from practical 2, let's execute the following steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. prepare the data\n",
    "y = df_nonlinear['y']\n",
    "X = df_nonlinear['X'][:, np.newaxis]\n",
    "X=(X-X.mean())/X.std()\n",
    "\n",
    "# 2. fit a linear model to the data\n",
    "lmod = LinearRegression()\n",
    "lmod.fit(X, y);\n",
    "\n",
    "# 3. make predictions on the data\n",
    "y_pred = lmod.predict(X)\n",
    "\n",
    "# 4. visualise the results.\n",
    "results = pd.DataFrame({'X':X[:,0],'y':y,'y_pred':y_pred})\n",
    "ax1 = results.plot.scatter(x='X', y='y');\n",
    "ax2 = results.plot.scatter(x='X', y='y_pred', ax=ax1, c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the Root Mean Squared Error (RMSE) on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(y, y_pred)))\n",
    "print(f\"Linear model: root mean squared error = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the linear model is a good fit?\n",
    "\n",
    "The best way to answer this question is to try to fit *other* types of models and see if they do better or worse when compared to the linear model with respect to performance on the RMSE.\n",
    "\n",
    "Before we do this, let's review some math."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of linear and nonlinear models \n",
    "Given a one-dimensional $x$ and $y$, the relationship between $x$ and $y$ is *linear* if we can write the relationship as the equation:\n",
    "\n",
    "$$ y = a x + b $$\n",
    "\n",
    "where $a$ and $b$ represent the *gradient* and the *intercept* of the linear model.\n",
    "\n",
    "Indeed, we can print out the gradient and intercecpt of the linear model, `lmod`, as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Linear model: gradient = {lmod.coef_[0]}\")\n",
    "print(f\"Linear model: intercept = {lmod.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relationship between $x$ and $y$ is *non-linear* simply if the relationship is not of the form $y = ax + b$. Some examples:\n",
    "1. $y = ae^{bx}$\n",
    "2. $y = a_{1} + a_{2}x + a_{3}x^{2} + a_{4}x^3$ + ...\n",
    "\n",
    "The relationship between $x$ and $y$ is *exponential* if we can write the relationship as equation (1), and the relationship between $x$ and $y$ is *polynomial* if we can write the relationship as equation (2). Both are examples of one-dimensional *non-linear* models.\n",
    "\n",
    "Let's try fitting a polynomial model to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit a *non-linear* model to *non-linear* data\n",
    "The first non-linear model we will explore will be the model described in (2). Looking at equation (2) we see that it is in fact a *linear model with polynomial features*. This we can use the same linear regression algorithm from before, but applied to a non-linear mapping of the original data. This is a common approach you will see all throughout machine learning.\n",
    "\n",
    "Let's first compute and visualise some polynomial features up to degree $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cubic = PolynomialFeatures(3).fit_transform(X)\n",
    "\n",
    "feature_names = ['1', 'x', 'x^2', 'x^3']\n",
    "df_X_cubic = pd.DataFrame(X_cubic, columns=feature_names)\n",
    "\n",
    "_, axes = plt.subplots(1, 4, figsize=(10,2))\n",
    "for i in range(len(feature_names)):\n",
    "    df_X_cubic.plot.scatter(x='x', y=feature_names[i],ax=axes[i],xticks=[],yticks=[]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit the linear model from before, but to the above polynomial (cubic) features rather than the raw values of $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmod2 = LinearRegression()\n",
    "lmod2.fit(X_cubic, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the regression model to make predictions on the data and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the data\n",
    "y_pred2 = lmod2.predict(X_cubic)\n",
    "\n",
    "# visualise the results\n",
    "results = pd.DataFrame({'X':X[:,0],'y':y,'y_pred':y_pred2})\n",
    "ax1 = results.plot.scatter(x='X', y='y');\n",
    "ax2 = results.plot.scatter(x='X', y='y_pred', ax=ax1, c='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the cubic model looks a better fit than the linear model? \n",
    "\n",
    "To assess if the cubic model is a *better fit* to the data than the linear model, we can compare the root mean squared error (RMSE) of each model on the data. Let's compute the RMSE of the cubic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(y, y_pred2)))\n",
    "print(f\"Cubic model: root mean squared error = {rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it compare to the RMSE of the linear model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
